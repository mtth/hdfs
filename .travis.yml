language: python
python:
  - 3.8
  - 3.7
  - 3.6
  - 3.5
  - 2.7
install:
  - pip install .[avro] coverage mock
before_script:
  - export HADOOP_HOME="$(./scripts/hadoop.sh download)"
  - export HADOOP_CONF_DIR="$(./scripts/hadoop.sh config)"
  - ./scripts/hadoop.sh start
  - export WEBHDFS_URL="http://$("${HADOOP_HOME}/bin/hdfs" getconf -confKey dfs.namenode.http-address)"
  - export HTTPFS_URL=http://localhost:14000
  - env | sort
  - sleep 5 # TODO: Find a better way to wait for all datanodes to become reachable.
script:
  - HDFSCLI_TEST_URL="$WEBHDFS_URL" nosetests --with-coverage --cover-package=hdfs
  - HDFSCLI_TEST_URL="$HTTPFS_URL" nosetests --with-coverage --cover-package=hdfs
after_script:
  - ./scripts/hadoop.sh stop
  - rm -r "$(dirname "$HADOOP_HOME")" "$HADOOP_CONF_DIR"
jobs:
  include:
    - stage: deploy
      if: tag =~ ^[\d\.]+$
      before_script: skip
      script: skip
      after_script: skip
      deploy:
        provider: pypi
        user: mtth
        password:
          secure: Oslw+ounXMBnCgUTF5wfXXvt/Tdxpm+pawVCP9EXdQyTO9zqmZ+aydXYhXl8Tlon/Cf4nHUWw43J67/SUx/gozSrkSRfsZ1kA9I9J6SvaxQpdHGwKTEFGuWVHNIwfRgeJ8GGPr8lBCG2LSnhRu1YPDO6R/jPdbcFbi7wMIAPjQA=
        on:
          repo: mtth/hdfs
          tags: true
