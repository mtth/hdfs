language: python
python:
  - 2.7
  - 3.3
  - 3.4
  - 3.5
  - 3.6
  - 3.7
install:
  - pip install . fastavro==0.9.6 coverage mock
before_script:
  - export HADOOP_HOME="$(./scripts/hadoop.sh download)"
  - export HADOOP_CONF_DIR="$(./scripts/hadoop.sh config)"
  - ./scripts/hadoop.sh start
  - export WEBHDFS_URL="http://$("${HADOOP_HOME}/bin/hdfs" getconf -confKey dfs.namenode.http-address)"
  - export HTTPFS_URL=http://localhost:14000
  - env | sort
  - sleep 5 # TODO: Find a better way to wait for all datanodes to become reachable.
script:
  - HDFSCLI_TEST_URL="$WEBHDFS_URL" nosetests --with-coverage --cover-package=hdfs
  - HDFSCLI_TEST_URL="$HTTPFS_URL" nosetests --with-coverage --cover-package=hdfs
after_script:
  - ./scripts/hadoop.sh stop
  - rm -r "$(dirname "$HADOOP_HOME")" "$HADOOP_CONF_DIR"
deploy:
  provider: pypi
  user: mtth
  password:
    secure: pJUxlxpB92jDCsAaMP67opcnu61A78BO02qhpvsdkXwCpJFBNBitgvqc8pfD1yCvkIPnCMjdhFKkJEUAqf3aCwAsLiCtL0xWqdwTpb9Ae3BkOsV4CpZZL3jkWAjiPWaPjZi3wbfJoSelTPB9NL6LuJ4KUfbBo4S1esQ1B3vCKuI=
  on:
    tags: true
